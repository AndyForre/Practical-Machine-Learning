##Assignment - Prediction Assignment Writeup report

###1. Data import and selection of predictors

The first part of this report describes selection of predictor variables for use during model construction. Reducing the number of predictor variables is an 
important step in order to remove random noise in the dataset. Several different strategies for reducing number of features exists. Below follows an explanation of 
the strategy used in this assignment.

The training dataset for the assignment was obtained from the following url: https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv. Data was 
downloaded and imported into R studio using the following code:

```{r}
  url1<- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
	download.file(url1,destfile="./trainset.csv")
	training = read.csv("trainset.csv", header=T, na.strings = c("NA","", " ","#DIV/0!") )
	url2<- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv "
	download.file(url2,destfile="./validationset.csv")
	testing = read.csv("validationset.csv",header=T, na.strings = c("NA","", " ","#DIV/0!") )
	
```
The trainingdata is a matrix comprising 19622 observations of 160 variables. The first step to select important predictor variables was to remove predictors that did 
not contain any values, i.e. the columns containing the NA values. The following code was used to remove predictors containing more than one NA value: 

```{r}
  RemovedNATrain <-which(colSums(is.na(training))>1)
	training1 <-training[,-c(RemovedNATrain)]
	dim(training1)
```
A total of 100 predictors were removed. The new data matrix comprised 19622 observations of 60 variables. Next step in predictor selection involved manual inspection of the predictors. The first seven predictors contained annotation information, hence likely not related to the predicted outcome. These were removed using the following code:

```{r}
training2 <- subset( training1, select = -c( X : num_window ))
```
The new data matrix comprised 19622 observations of 53 variables. Next step involved identification of near zero variance predictors, since these would most 
certainly not be useful covariates:

```{r}
library(caret)
TestNearZero <- nearZeroVar(training2[sapply(training2,is.numeric)],saveMetrics = TRUE)
nzv(training2, freqCut = 95/5, uniqueCut = 10, saveMetrics = FALSE, names = FALSE)
TestNearZero
```
No samples were removed, all samples had enough variability. Next step was to remove highly correlated numerical features. This were used by first generating a 
correlation matrix and then use the findCorrelation function within the caret package to remove highly correlated predictors:

```{r}
CorMat <-abs(cor(training2[sapply(training2, is.numeric)]))
r = findCorrelation(CorMat, cutoff = .90, verbose = TRUE)
training3 = training2[,-r]
```
The new data matrix comprised 19622 observations of 46 variables. This was the last step in the predictor selection process. The 46 predictor variables were used as 
input into the model. 

###2. Construction and optimization of prediction model

The second part of this report describes construction of the predictive model. In order to achieve accurate predictions of unknown samples, it is important to tune 
and optimize parameters on a selected model. It is important that all such optimization steps are performed only on samples in training data in order not to risk 
overfitting when generalizing to new samples. In order to tune our model and to estimate model performance, we split the trainingdata into two fractions: a training 
fraction (75% of samples) containing the samples used to train the model and to estimate the tuning parameters, and a test set (25% of samples) to evaluate the 
ability of our model to generalize to unseen samples.

```{r}
set.seed(2) 
train <-createDataPartition(y=training3$classe,p=0.75,list=FALSE)
trainset<-training3[train,]	
testset<-training3[-train,]
dim(trainset)
dim(testset)
```

Since the variable that should be predicted in this assignment comprised five groups, binary classification models such as SVMs could not be considered. Due to 
personal preferences, a Random Forest (RF) model was first fitted to the samples in the trainingset. The only available tuning parameter was mtry, the number of 
variables randomly sampled as candidates at each split. 
```{r}
library(randomForest)
set.seed(2)
ctrl = trainControl(method = "cv",number=3, allowParallel=T)
rf.caret.model = train(classe~., data=trainset, method="rf",trControl=ctrl, tuneGrid=expand.grid(.mtry=1:7),verbose=FALSE)
rf.caret.model
rf.caret.model$finalModel
plot(rf.caret.model)
```
We can also chech the most important variables in case we need to reduce number of predictors:

```{r}
varImp(rf.caret.model)
```
Due to limitations in time, a 3-fold cross-validation procedure was first used on samples in trainingset to determine the mtry parameter. The number of mtry to 
evaluate was set to all integers between one and the square root of the number of predictors (???46 = 7). The best performing value of mtry was determined based on 
highest accuracy. This step could be repeated if the performance on test samples was not sufficient.

###3.) Results - Prediction of samples in test set

Samples in the testset set were predicted using the tuned model.

```{r}
pred1 = predict(rf.caret.model, newdata=testset)
confusionMatrix(pred1, testset$classe)
```
For prediction of samples in the left out test set, the model reached an impressive accuracy of 99.5%. This means that we do not need to optimize model or try other 
models. Since compounds in left out test set was not used during model construction or optimization, the accuracy is a valid estimate of the out of sample accuracy. 



###4.) Results - Prediction of samples in independent validationset

Samples in the independent test set were predicted with the same model:

```{r}
Finaltest<-testing[,names(training3[,-46])]
dim(Finaltest)
pred2 = predict(rf.caret.model, newdata=Finaltest)
```
These answers were submitted to Coursera